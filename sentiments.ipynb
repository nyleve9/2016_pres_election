{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (12, 9) #<--- large default figures\n",
    "\n",
    "# Plot text elements\n",
    "rcParams['axes.labelsize'] = 17\n",
    "rcParams['axes.titlesize'] = 17\n",
    "rcParams['xtick.labelsize'] = 15\n",
    "rcParams['ytick.labelsize'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evelyn/Workspaces/springboard/capstone/electiontweets\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/evelyn/Workspaces/springboard/capstone/electiontweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_files = ['#makeamericagreatagain_2016-11-10_to_2016-11-14.json',\n",
    "               '#makeamericagreatagain_2016-11-09.json',\n",
    "               '#makeamericagreatagain_2016-11-08.json',\n",
    "               '#makeamericagreatagain_2016-11-07.json',\n",
    "               '#makeamericagreatagain_2016-11-06.json',\n",
    "               '#makeamericagreatagain_2016-11-05.json',\n",
    "               '#makeamericagreatagain_2016-11-04.json',\n",
    "               '#makeamericagreatagain_2016-11-01.json',\n",
    "               '#makeamericagreatagain_2016-10-31.json']\n",
    "\n",
    "tweets = []\n",
    "for file in tweet_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158160"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'coordinates': None,\n",
       " 'created_at': 'Mon Nov 14 23:59:45 +0000 2016',\n",
       " 'entities': {'hashtags': [{'indices': [47, 69],\n",
       "    'text': 'MakeAmericaGreatAgain'}],\n",
       "  'media': [{'display_url': 'pic.twitter.com/QK4tkIOBHY',\n",
       "    'expanded_url': 'https://twitter.com/The_Trump_Train/status/798299732451950600/photo/1',\n",
       "    'id': 798299726424707076,\n",
       "    'id_str': '798299726424707076',\n",
       "    'indices': [70, 93],\n",
       "    'media_url': 'http://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "    'sizes': {'large': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "     'medium': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "     'small': {'h': 240, 'resize': 'fit', 'w': 680},\n",
       "     'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "    'source_status_id': 798299732451950600,\n",
       "    'source_status_id_str': '798299732451950600',\n",
       "    'source_user_id': 4102376488,\n",
       "    'source_user_id_str': '4102376488',\n",
       "    'type': 'photo',\n",
       "    'url': 'https://t.co/QK4tkIOBHY'}],\n",
       "  'symbols': [],\n",
       "  'urls': [],\n",
       "  'user_mentions': [{'id': 4102376488,\n",
       "    'id_str': '4102376488',\n",
       "    'indices': [3, 19],\n",
       "    'name': 'The Trump Train',\n",
       "    'screen_name': 'The_Trump_Train'}]},\n",
       " 'extended_entities': {'media': [{'display_url': 'pic.twitter.com/QK4tkIOBHY',\n",
       "    'expanded_url': 'https://twitter.com/The_Trump_Train/status/798299732451950600/photo/1',\n",
       "    'id': 798299726424707076,\n",
       "    'id_str': '798299726424707076',\n",
       "    'indices': [70, 93],\n",
       "    'media_url': 'http://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "    'sizes': {'large': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "     'medium': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "     'small': {'h': 240, 'resize': 'fit', 'w': 680},\n",
       "     'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "    'source_status_id': 798299732451950600,\n",
       "    'source_status_id_str': '798299732451950600',\n",
       "    'source_user_id': 4102376488,\n",
       "    'source_user_id_str': '4102376488',\n",
       "    'type': 'photo',\n",
       "    'url': 'https://t.co/QK4tkIOBHY'}]},\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'geo': None,\n",
       " 'id': 798314509337235456,\n",
       " 'id_str': '798314509337235456',\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'is_quote_status': False,\n",
       " 'lang': 'en',\n",
       " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       " 'place': None,\n",
       " 'possibly_sensitive': False,\n",
       " 'retweet_count': 307,\n",
       " 'retweeted': False,\n",
       " 'retweeted_status': {'contributors': None,\n",
       "  'coordinates': None,\n",
       "  'created_at': 'Mon Nov 14 23:01:02 +0000 2016',\n",
       "  'entities': {'hashtags': [{'indices': [26, 48],\n",
       "     'text': 'MakeAmericaGreatAgain'}],\n",
       "   'media': [{'display_url': 'pic.twitter.com/QK4tkIOBHY',\n",
       "     'expanded_url': 'https://twitter.com/The_Trump_Train/status/798299732451950600/photo/1',\n",
       "     'id': 798299726424707076,\n",
       "     'id_str': '798299726424707076',\n",
       "     'indices': [49, 72],\n",
       "     'media_url': 'http://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "     'media_url_https': 'https://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "     'sizes': {'large': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "      'medium': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "      'small': {'h': 240, 'resize': 'fit', 'w': 680},\n",
       "      'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "     'type': 'photo',\n",
       "     'url': 'https://t.co/QK4tkIOBHY'}],\n",
       "   'symbols': [],\n",
       "   'urls': [],\n",
       "   'user_mentions': []},\n",
       "  'extended_entities': {'media': [{'display_url': 'pic.twitter.com/QK4tkIOBHY',\n",
       "     'expanded_url': 'https://twitter.com/The_Trump_Train/status/798299732451950600/photo/1',\n",
       "     'id': 798299726424707076,\n",
       "     'id_str': '798299726424707076',\n",
       "     'indices': [49, 72],\n",
       "     'media_url': 'http://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "     'media_url_https': 'https://pbs.twimg.com/media/CxQhXyHWEAQdbmv.jpg',\n",
       "     'sizes': {'large': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "      'medium': {'h': 265, 'resize': 'fit', 'w': 750},\n",
       "      'small': {'h': 240, 'resize': 'fit', 'w': 680},\n",
       "      'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "     'type': 'photo',\n",
       "     'url': 'https://t.co/QK4tkIOBHY'}]},\n",
       "  'favorite_count': 829,\n",
       "  'favorited': False,\n",
       "  'geo': None,\n",
       "  'id': 798299732451950600,\n",
       "  'id_str': '798299732451950600',\n",
       "  'in_reply_to_screen_name': None,\n",
       "  'in_reply_to_status_id': None,\n",
       "  'in_reply_to_status_id_str': None,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'is_quote_status': False,\n",
       "  'lang': 'en',\n",
       "  'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       "  'place': None,\n",
       "  'possibly_sensitive': False,\n",
       "  'retweet_count': 307,\n",
       "  'retweeted': False,\n",
       "  'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       "  'text': 'The best picture of 2016! #MakeAmericaGreatAgain https://t.co/QK4tkIOBHY',\n",
       "  'truncated': False,\n",
       "  'user': {'contributors_enabled': False,\n",
       "   'created_at': 'Tue Nov 03 05:07:18 +0000 2015',\n",
       "   'default_profile': True,\n",
       "   'default_profile_image': False,\n",
       "   'description': 'The official Twitter network for Donald Trump and The Trump Train movement! #MakeAmericaGreatAgain',\n",
       "   'entities': {'description': {'urls': []}},\n",
       "   'favourites_count': 6061,\n",
       "   'follow_request_sent': False,\n",
       "   'followers_count': 87877,\n",
       "   'following': False,\n",
       "   'friends_count': 89989,\n",
       "   'geo_enabled': False,\n",
       "   'has_extended_profile': False,\n",
       "   'id': 4102376488,\n",
       "   'id_str': '4102376488',\n",
       "   'is_translation_enabled': False,\n",
       "   'is_translator': False,\n",
       "   'lang': 'en',\n",
       "   'listed_count': 281,\n",
       "   'location': 'The White House',\n",
       "   'name': 'The Trump Train',\n",
       "   'notifications': False,\n",
       "   'profile_background_color': 'C0DEED',\n",
       "   'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "   'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "   'profile_background_tile': False,\n",
       "   'profile_banner_url': 'https://pbs.twimg.com/profile_banners/4102376488/1452492091',\n",
       "   'profile_image_url': 'http://pbs.twimg.com/profile_images/686427697887731713/q0XMtMoW_normal.jpg',\n",
       "   'profile_image_url_https': 'https://pbs.twimg.com/profile_images/686427697887731713/q0XMtMoW_normal.jpg',\n",
       "   'profile_link_color': '1DA1F2',\n",
       "   'profile_sidebar_border_color': 'C0DEED',\n",
       "   'profile_sidebar_fill_color': 'DDEEF6',\n",
       "   'profile_text_color': '333333',\n",
       "   'profile_use_background_image': True,\n",
       "   'protected': False,\n",
       "   'screen_name': 'The_Trump_Train',\n",
       "   'statuses_count': 2414,\n",
       "   'time_zone': None,\n",
       "   'translator_type': 'none',\n",
       "   'url': None,\n",
       "   'utc_offset': None,\n",
       "   'verified': False}},\n",
       " 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " 'text': 'RT @The_Trump_Train: The best picture of 2016! #MakeAmericaGreatAgain https://t.co/QK4tkIOBHY',\n",
       " 'truncated': False,\n",
       " 'user': {'contributors_enabled': False,\n",
       "  'created_at': 'Wed Jan 27 14:16:43 +0000 2016',\n",
       "  'default_profile': True,\n",
       "  'default_profile_image': False,\n",
       "  'description': '',\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'favourites_count': 16079,\n",
       "  'follow_request_sent': False,\n",
       "  'followers_count': 459,\n",
       "  'following': False,\n",
       "  'friends_count': 94,\n",
       "  'geo_enabled': True,\n",
       "  'has_extended_profile': False,\n",
       "  'id': 4853583291,\n",
       "  'id_str': '4853583291',\n",
       "  'is_translation_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'lang': 'en',\n",
       "  'listed_count': 143,\n",
       "  'location': '',\n",
       "  'name': 'Suzanne Rivkin',\n",
       "  'notifications': False,\n",
       "  'profile_background_color': 'F5F8FA',\n",
       "  'profile_background_image_url': None,\n",
       "  'profile_background_image_url_https': None,\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/774347307051614208/oZNIM1GC_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/774347307051614208/oZNIM1GC_normal.jpg',\n",
       "  'profile_link_color': '1DA1F2',\n",
       "  'profile_sidebar_border_color': 'C0DEED',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'protected': False,\n",
       "  'screen_name': 'rivkin_suzanne',\n",
       "  'statuses_count': 42980,\n",
       "  'time_zone': None,\n",
       "  'translator_type': 'none',\n",
       "  'url': None,\n",
       "  'utc_offset': None,\n",
       "  'verified': False}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetdf=pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweetdf=tweetdf[tweetdf.lang=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Nov 14 23:59:45 +0000 2016</td>\n",
       "      <td>{'user_mentions': [{'id_str': '4102376488', 'n...</td>\n",
       "      <td>{'media': [{'type': 'photo', 'source_user_id_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>798314509337235456</td>\n",
       "      <td>798314509337235456</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>307</td>\n",
       "      <td>False</td>\n",
       "      <td>{'text': 'The best picture of 2016! #MakeAmeri...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @The_Trump_Train: The best picture of 2016!...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'protected': False, 'has_extended_profile': F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Nov 14 23:59:44 +0000 2016</td>\n",
       "      <td>{'user_mentions': [{'id_str': '90480218', 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>798314503779786754</td>\n",
       "      <td>798314503779786754</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Will @RichardGrenell have a role in the Trump ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'protected': False, 'has_extended_profile': F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Nov 14 23:59:38 +0000 2016</td>\n",
       "      <td>{'user_mentions': [{'id_str': '25429371', 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>798314479985557504</td>\n",
       "      <td>798314479985557504</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7153</td>\n",
       "      <td>False</td>\n",
       "      <td>{'text': 'RT if you supported @realDonaldTrump...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>RT @KatrinaPierson: RT if you supported @realD...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'protected': False, 'has_extended_profile': T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Nov 14 23:59:38 +0000 2016</td>\n",
       "      <td>{'user_mentions': [{'id_str': '1618507111', 'n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>798314479339585536</td>\n",
       "      <td>798314479339585536</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>{'text': 'Sorry #ObamaPresser we're not doing ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @WillaimShat: Sorry #ObamaPresser we're not...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'protected': False, 'has_extended_profile': T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Nov 14 23:59:32 +0000 2016</td>\n",
       "      <td>{'user_mentions': [{'id_str': '3001538578', 'n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>798314452101853184</td>\n",
       "      <td>798314452101853184</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@KMWalsh_GOP UNITED States of America!!! #stro...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'protected': False, 'has_extended_profile': F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at  \\\n",
       "0         None        None  Mon Nov 14 23:59:45 +0000 2016   \n",
       "1         None        None  Mon Nov 14 23:59:44 +0000 2016   \n",
       "2         None        None  Mon Nov 14 23:59:38 +0000 2016   \n",
       "3         None        None  Mon Nov 14 23:59:38 +0000 2016   \n",
       "4         None        None  Mon Nov 14 23:59:32 +0000 2016   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'user_mentions': [{'id_str': '4102376488', 'n...   \n",
       "1  {'user_mentions': [{'id_str': '90480218', 'nam...   \n",
       "2  {'user_mentions': [{'id_str': '25429371', 'nam...   \n",
       "3  {'user_mentions': [{'id_str': '1618507111', 'n...   \n",
       "4  {'user_mentions': [{'id_str': '3001538578', 'n...   \n",
       "\n",
       "                                   extended_entities  favorite_count  \\\n",
       "0  {'media': [{'type': 'photo', 'source_user_id_s...               0   \n",
       "1                                                NaN               0   \n",
       "2                                                NaN               0   \n",
       "3                                                NaN               0   \n",
       "4                                                NaN               0   \n",
       "\n",
       "  favorited   geo                  id              id_str  \\\n",
       "0     False  None  798314509337235456  798314509337235456   \n",
       "1     False  None  798314503779786754  798314503779786754   \n",
       "2     False  None  798314479985557504  798314479985557504   \n",
       "3     False  None  798314479339585536  798314479339585536   \n",
       "4     False  None  798314452101853184  798314452101853184   \n",
       "\n",
       "                         ...                         quoted_status  \\\n",
       "0                        ...                                   NaN   \n",
       "1                        ...                                   NaN   \n",
       "2                        ...                                   NaN   \n",
       "3                        ...                                   NaN   \n",
       "4                        ...                                   NaN   \n",
       "\n",
       "   quoted_status_id quoted_status_id_str  retweet_count retweeted  \\\n",
       "0               NaN                  NaN            307     False   \n",
       "1               NaN                  NaN              0     False   \n",
       "2               NaN                  NaN           7153     False   \n",
       "3               NaN                  NaN              3     False   \n",
       "4               NaN                  NaN              0     False   \n",
       "\n",
       "                                    retweeted_status  \\\n",
       "0  {'text': 'The best picture of 2016! #MakeAmeri...   \n",
       "1                                                NaN   \n",
       "2  {'text': 'RT if you supported @realDonaldTrump...   \n",
       "3  {'text': 'Sorry #ObamaPresser we're not doing ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com/#!/download/ipad\" ...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                text truncated  \\\n",
       "0  RT @The_Trump_Train: The best picture of 2016!...     False   \n",
       "1  Will @RichardGrenell have a role in the Trump ...     False   \n",
       "2  RT @KatrinaPierson: RT if you supported @realD...     False   \n",
       "3  RT @WillaimShat: Sorry #ObamaPresser we're not...     False   \n",
       "4  @KMWalsh_GOP UNITED States of America!!! #stro...     False   \n",
       "\n",
       "                                                user  \n",
       "0  {'protected': False, 'has_extended_profile': F...  \n",
       "1  {'protected': False, 'has_extended_profile': F...  \n",
       "2  {'protected': False, 'has_extended_profile': T...  \n",
       "3  {'protected': False, 'has_extended_profile': T...  \n",
       "4  {'protected': False, 'has_extended_profile': F...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135092"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweetdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158155</th>\n",
       "      <td>793045213505789952</td>\n",
       "      <td>RT @WeNeedTrump: Post game coverage of the Cub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158156</th>\n",
       "      <td>793045207369576448</td>\n",
       "      <td>RT @EricaMelone: @donnabrazile Please golden, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158157</th>\n",
       "      <td>793045188700676096</td>\n",
       "      <td>RT @TeresaEdelglass: \"I'm going to deliver the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158158</th>\n",
       "      <td>793045160544337920</td>\n",
       "      <td>RT @WeNeedTrump: Please RETWEET! This video co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158159</th>\n",
       "      <td>793045113542840321</td>\n",
       "      <td>Eric Holder's Endorsement Is the KISS OF DEATH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tweetid                                              tweet\n",
       "158155  793045213505789952  RT @WeNeedTrump: Post game coverage of the Cub...\n",
       "158156  793045207369576448  RT @EricaMelone: @donnabrazile Please golden, ...\n",
       "158157  793045188700676096  RT @TeresaEdelglass: \"I'm going to deliver the...\n",
       "158158  793045160544337920  RT @WeNeedTrump: Please RETWEET! This video co...\n",
       "158159  793045113542840321  Eric Holder's Endorsement Is the KISS OF DEATH..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringdf = pd.DataFrame()\n",
    "stringdf['tweetid'] = tweetdf['id']\n",
    "stringdf['tweet'] = tweetdf['text']\n",
    "stringdf.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135092"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stringdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135092"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringdf.tweetid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from https://www.ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\n",
    "\n",
    "* Lower Case - Convert the tweets to lower case.\n",
    "* URLs - I don't intend to follow the short urls and determine the content of the site, so we can eliminate all of these URLs via regular expression matching or replace with generic word URL.\n",
    "* @username - we can eliminate \"@username\" via regex matching or replace it with generic word AT_USER.\n",
    "* #hashtag - hash tags can give us some useful information, so it is useful to replace them with the exact same word without the hash. E.g. #nike replaced with 'nike'.\n",
    "* Punctuations and additional white spaces - remove punctuation at the start and ending of the tweets. E.g: ' the day is beautiful! ' replaced with 'the day is beautiful'. It is also helpful to replace multiple whitespaces with a single whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import regex\n",
    "import re\n",
    "\n",
    "#start process_tweet\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stringdf.tweet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This processes text, but the indices are all \"0\"\n",
    "\n",
    "processdf = pd.DataFrame(columns=['processed'])\n",
    "for row in stringdf.text[:5]:\n",
    "    df=pd.DataFrame([[processTweet(row)]], columns=['processed'])\n",
    "    processdf = processdf.append(df)\n",
    "\n",
    "processdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a much better way, much faster\n",
    "\n",
    "processed = pd.DataFrame(stringdf.tweet.apply(lambda row: processTweet(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt AT_USER the best picture of 2016! makeameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will AT_USER have a role in the trump presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt AT_USER rt if you supported AT_USER from da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt AT_USER sorry obamapresser we're not doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT_USER united states of america!!! strongerto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  rt AT_USER the best picture of 2016! makeameri...\n",
       "1  will AT_USER have a role in the trump presiden...\n",
       "2  rt AT_USER rt if you supported AT_USER from da...\n",
       "3  rt AT_USER sorry obamapresser we're not doing ...\n",
       "4  AT_USER united states of america!!! strongerto..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135092"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158155</th>\n",
       "      <td>rt AT_USER post game coverage of the cubs game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158156</th>\n",
       "      <td>rt AT_USER AT_USER please golden, sweet jesus....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158157</th>\n",
       "      <td>rt AT_USER \"i'm going to deliver the honest go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158158</th>\n",
       "      <td>rt AT_USER please retweet! this video could wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158159</th>\n",
       "      <td>eric holder's endorsement is the kiss of death...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "158155  rt AT_USER post game coverage of the cubs game...\n",
       "158156  rt AT_USER AT_USER please golden, sweet jesus....\n",
       "158157  rt AT_USER \"i'm going to deliver the honest go...\n",
       "158158  rt AT_USER please retweet! this video could wi...\n",
       "158159  eric holder's endorsement is the kiss of death..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.columns=['text']\n",
    "processed.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158150</th>\n",
       "      <td>793045291200937984</td>\n",
       "      <td>RT @WesRichardsonNY: Trump's tax cuts are exac...</td>\n",
       "      <td>rt AT_USER trump's tax cuts are exactly what a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158151</th>\n",
       "      <td>793045277234044929</td>\n",
       "      <td>RT @WeNeedTrump: Please RETWEET! This video co...</td>\n",
       "      <td>rt AT_USER please retweet! this video could wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158152</th>\n",
       "      <td>793045256698601472</td>\n",
       "      <td>RT @EricTrump: Always love being with you @Jud...</td>\n",
       "      <td>rt AT_USER always love being with you AT_USER ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158153</th>\n",
       "      <td>793045249153200128</td>\n",
       "      <td>RT @EricTrump: On behalf of the entire family,...</td>\n",
       "      <td>rt AT_USER on behalf of the entire family, we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>793045237585285122</td>\n",
       "      <td>RT @DonaldJTrumpJr: Too funny to not share. Fr...</td>\n",
       "      <td>rt AT_USER too funny to not share. from AT_USE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158155</th>\n",
       "      <td>793045213505789952</td>\n",
       "      <td>RT @WeNeedTrump: Post game coverage of the Cub...</td>\n",
       "      <td>rt AT_USER post game coverage of the cubs game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158156</th>\n",
       "      <td>793045207369576448</td>\n",
       "      <td>RT @EricaMelone: @donnabrazile Please golden, ...</td>\n",
       "      <td>rt AT_USER AT_USER please golden, sweet jesus....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158157</th>\n",
       "      <td>793045188700676096</td>\n",
       "      <td>RT @TeresaEdelglass: \"I'm going to deliver the...</td>\n",
       "      <td>rt AT_USER \"i'm going to deliver the honest go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158158</th>\n",
       "      <td>793045160544337920</td>\n",
       "      <td>RT @WeNeedTrump: Please RETWEET! This video co...</td>\n",
       "      <td>rt AT_USER please retweet! this video could wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158159</th>\n",
       "      <td>793045113542840321</td>\n",
       "      <td>Eric Holder's Endorsement Is the KISS OF DEATH...</td>\n",
       "      <td>eric holder's endorsement is the kiss of death...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tweetid                                              tweet  \\\n",
       "158150  793045291200937984  RT @WesRichardsonNY: Trump's tax cuts are exac...   \n",
       "158151  793045277234044929  RT @WeNeedTrump: Please RETWEET! This video co...   \n",
       "158152  793045256698601472  RT @EricTrump: Always love being with you @Jud...   \n",
       "158153  793045249153200128  RT @EricTrump: On behalf of the entire family,...   \n",
       "158154  793045237585285122  RT @DonaldJTrumpJr: Too funny to not share. Fr...   \n",
       "158155  793045213505789952  RT @WeNeedTrump: Post game coverage of the Cub...   \n",
       "158156  793045207369576448  RT @EricaMelone: @donnabrazile Please golden, ...   \n",
       "158157  793045188700676096  RT @TeresaEdelglass: \"I'm going to deliver the...   \n",
       "158158  793045160544337920  RT @WeNeedTrump: Please RETWEET! This video co...   \n",
       "158159  793045113542840321  Eric Holder's Endorsement Is the KISS OF DEATH...   \n",
       "\n",
       "                                                     text  \n",
       "158150  rt AT_USER trump's tax cuts are exactly what a...  \n",
       "158151  rt AT_USER please retweet! this video could wi...  \n",
       "158152  rt AT_USER always love being with you AT_USER ...  \n",
       "158153  rt AT_USER on behalf of the entire family, we ...  \n",
       "158154  rt AT_USER too funny to not share. from AT_USE...  \n",
       "158155  rt AT_USER post game coverage of the cubs game...  \n",
       "158156  rt AT_USER AT_USER please golden, sweet jesus....  \n",
       "158157  rt AT_USER \"i'm going to deliver the honest go...  \n",
       "158158  rt AT_USER please retweet! this video could wi...  \n",
       "158159  eric holder's endorsement is the kiss of death...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processdf=stringdf.join(processed)\n",
    "processdf.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_index() missing 1 required positional argument: 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-5e71a65ffb5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: set_index() missing 1 required positional argument: 'keys'"
     ]
    }
   ],
   "source": [
    "processdf.set_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35237"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processdf.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135092"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processdf.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Filtering tweet words (for feature vector)\n",
    "\n",
    "* Stop words - a, is, the, with etc. The full list of stop words can be found at Stop Word List. These words don't indicate any sentiment and can be removed.\n",
    "* Repeating letters - if you look at the tweets, sometimes people repeat letters to stress the emotion. E.g. hunggrryyy, huuuuuuungry for 'hungry'. We can look for 2 or more repetitive letters in words and replace them by 2 of the same.\n",
    "* Punctuation - we can remove punctuation such as comma, single/double quote, question marks at the start and end of each word. E.g. beautiful!!!!!! replaced with beautiful\n",
    "* Words must start with an alphabet - For simplicity sake, we can remove all those words which don't start with an alphabet. E.g. 15th, 5.34am\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'picture', 'makeamericagreatagain', 'url']\n",
      "['role', 'trump', 'presidency', 'oh', 'surely', 'hope', 'classy', 'guy', 'trump', 'makeamericagreatagain']\n",
      "['rt', 'rt', 'supported', 'day', 'cool', 'teamtrump', 'makeamericagreatagain']\n",
      "['rt', 'sorry', 'obamapresser', 'doing', 'makeamericagreatagain', 'include']\n",
      "['united', 'strongertogether', 'makeamericagreatagain', 'strongertogethermakesamericagreatagain']\n"
     ]
    }
   ],
   "source": [
    "#initialize stopWords\n",
    "stopWords = []\n",
    "\n",
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "#end\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word starts with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    "#end\n",
    "\n",
    "stopWords = getStopWordList('stopwordlist.txt')\n",
    "for row in processdf.text[:5]:\n",
    "    processedTweet = processTweet(row)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    #fullvector = featureVector.append()\n",
    "    print (featureVector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT_USER', 'URL', 'a', 'about', 'above']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = getStopWordList('stopwordlist.txt')\n",
    "stopWords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature extraction \n",
    "The following code, extracts the tweets and label from the csv file and processes it as outlined above and obtains a feature vector and stores it in a variable called \"tweets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['sad', 'apl', 'friend', '0'], '0'), None, (['missed', 'moon', 'trailer', '0'], '0'), None, (['omg', '1'], '1'), None, (['omgaga', 'im', 'soo', 'im', 'gunna', 'cry', 'dentist', 'suposed', 'crown', '0'], '0'), None, (['mi', 'bf', 'cheating', '0'], '0'), None, (['worry', '0'], '0'), None, (['juusst', '1'], '1'), None, (['sunny', 'tomorrow', 'tv', 'tonight', '0'], '0'), None, (['handed', 'uniform', 'miss', '1'], '1'), None, (['hmm', 'wonder', '1'], '1'), None]\n"
     ]
    }
   ],
   "source": [
    "# For the sample sentiments (n=100?), let alone whole sentiment list, keep getting\n",
    "# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 2525: invalid start byte\n",
    "\n",
    "f = open('subset_sentiments.csv', 'r')\n",
    "inpTweets = csv.reader(f, delimiter=',',quotechar='|')\n",
    "tweets = []\n",
    "for row in inpTweets:\n",
    "    sentiment=row[1]\n",
    "    tweet=row[3]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    tweets.append((featureVector, sentiment))\n",
    "    tline = featureVector.append(sentiment)\n",
    "    tweets.append(tline)\n",
    "    #print(featureVector, sentiment)\n",
    "print(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sad', 'apl', 'friend'], ['missed', 'moon', 'trailer'], ['omg'], ['omgaga', 'im', 'soo', 'im', 'gunna', 'cry', 'dentist', 'suposed', 'crown'], ['mi', 'bf', 'cheating'], ['worry'], ['juusst'], ['sunny', 'tomorrow', 'tv', 'tonight'], ['handed', 'uniform', 'miss'], ['hmm', 'wonder']]\n"
     ]
    }
   ],
   "source": [
    "#Read the tweets one by one, process it, append to list of processed tweets\n",
    "f = open('subset_sentiments.csv', 'r')\n",
    "inpTweets = csv.reader(f, delimiter=',',quotechar='|')\n",
    "tweet_list = []\n",
    "\n",
    "for row in inpTweets:\n",
    "    sentiment = row[1]\n",
    "    tweet = row[3]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    tweet_list.append((featureVector));\n",
    "#end loop\n",
    "print (tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature list\n",
    "To be used to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-09c5baf686fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatureList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-460d01d69715>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtweet_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "featureList = extract_features(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk extraction of features\n",
    "NLTK has a neat feature which enables to extract features as above in bulk for all the tweets and can be done using the below code snippet. The line of interest is \"nltk.classify.apply_features(extract_features, tweets)\" where you pass in the tweets variable to the extract_features method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the tweets one by one and process it\n",
    "#inpTweets = csv.reader(open('data/sampleTweets.csv', 'rb'), delimiter=',', quotechar='|')\n",
    "#stopWords = getStopWordList('data/feature_list/stopwords.txt')\n",
    "\n",
    "f = open('subset_sentiments.csv', 'r')\n",
    "inpTweets = csv.reader(f, delimiter=',',quotechar='|')\n",
    "featureList = []\n",
    "\n",
    "# Get tweet words\n",
    "tweets = []\n",
    "for row in inpTweets:\n",
    "    sentiment = row[1]\n",
    "    tweet = row[3]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    featureList.extend(featureVector)\n",
    "    tweets.append((featureVector, sentiment));\n",
    "#end loop\n",
    "\n",
    "# Remove featureList duplicates\n",
    "featureList = list(set(featureList))\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'contains(cry)': False, 'contains(gunna)': False, 'contains(sunny)': False, 'contains(friend)': True, 'contains(uniform)': False, 'contains(bf)': False, 'contains(suposed)': False, 'contains(worry)': False, 'contains(tv)': False, 'contains(tonight)': False, 'contains(miss)': False, 'contains(sad)': True, 'contains(im)': False, 'contains(moon)': False, 'contains(omgaga)': False, 'contains(apl)': True, 'contains(juusst)': False, 'contains(trailer)': False, 'contains(cheating)': False, 'contains(omg)': False, 'contains(hmm)': False, 'contains(missed)': False, 'contains(handed)': False, 'contains(dentist)': False, 'contains(tomorrow)': False, 'contains(crown)': False, 'contains(mi)': False, 'contains(soo)': False, 'contains(wonder)': False}, '0'), ({'contains(cry)': False, 'contains(gunna)': False, 'contains(sunny)': False, 'contains(friend)': False, 'contains(uniform)': False, 'contains(bf)': False, 'contains(suposed)': False, 'contains(worry)': False, 'contains(tv)': False, 'contains(tonight)': False, 'contains(miss)': False, 'contains(sad)': False, 'contains(im)': False, 'contains(moon)': True, 'contains(omgaga)': False, 'contains(apl)': False, 'contains(juusst)': False, 'contains(trailer)': True, 'contains(cheating)': False, 'contains(omg)': False, 'contains(hmm)': False, 'contains(missed)': True, 'contains(handed)': False, 'contains(dentist)': False, 'contains(tomorrow)': False, 'contains(crown)': False, 'contains(mi)': False, 'contains(soo)': False, 'contains(wonder)': False}, '0'), ...]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Instantiate a classifier and classify test tweets in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Test the classifier\n",
    "testTweet = 'Congrats @ravikiranj, i heard you wrote a new tech post on sentiment analysis'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "print (NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet))))\n",
    "\n",
    "#Output\n",
    "#======\n",
    "#positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(omg) = False               0 : 1      =      1.3 : 1.0\n",
      "        contains(handed) = False               0 : 1      =      1.3 : 1.0\n",
      "       contains(uniform) = False               0 : 1      =      1.3 : 1.0\n",
      "        contains(wonder) = False               0 : 1      =      1.3 : 1.0\n",
      "           contains(hmm) = False               0 : 1      =      1.3 : 1.0\n",
      "          contains(miss) = False               0 : 1      =      1.3 : 1.0\n",
      "        contains(juusst) = False               0 : 1      =      1.3 : 1.0\n",
      "         contains(sunny) = False               1 : 0      =      1.1 : 1.0\n",
      "       contains(dentist) = False               1 : 0      =      1.1 : 1.0\n",
      "      contains(cheating) = False               1 : 0      =      1.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print out the most informative features about the classifier\n",
    "print (NBClassifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do have a test set of manually labeled data, you can cross verify it via the classifier. You will soon find that the results are not so good as you expected (see below). This is essentially because in the training data didn't cover the words encountered in this tweet and the classifier has little knowledge to classify this tweet and most often the tweet gets assigned the default classification label, in this case happens to be 'positive'. Hence, training dataset is very crucial for the success of these classifiers. Anything below 10k of training tweets will give you pretty mediocre results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "testTweet = 'I am so badly hurt'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "print (NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet))))\n",
    "\n",
    "#Output\n",
    "#======\n",
    "#positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LazyCorpusLoader.__repr__ of <TwitterCorpusReader in '.../corpora/twitter_samples' (not loaded yet)>>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.unicode_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is taken from:\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read\n",
    "# the labeled training data\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a peek\n",
    "print train[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import BeautifulSoup into your workspace\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "# Initialize the BeautifulSoup object on a single movie review     \n",
    "example1 = BeautifulSoup(train[\"review\"][0])  \n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "print train[\"review\"][0]\n",
    "print example1.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for general steps in creating bag of words\n",
    "\n",
    "* list_BOW = []\n",
    "\n",
    "* For each review in the training set: Strip the newline character â€œnâ€ at the end of each review.\n",
    "\n",
    "* Place a space before and after each of the following characters: .,()[]:;â€  \n",
    "\n",
    "(This prevents sentences like â€œI like this book.It is engagingâ€ being interpreted as \n",
    "[â€œIâ€, â€œlikeâ€, â€œthisâ€, â€œbook.Itâ€, â€œisâ€, â€œengagingâ€].)\n",
    "\n",
    "* Tokenize the text by splitting it on spaces.\n",
    "\n",
    "* Remove tokens which consist of only a space, empty string or punctuation marks.\n",
    "\n",
    "* Append the tokens to list_BOW.\n",
    "list_BOW now contains all words occuring in the training set.\n",
    "\n",
    "* Place list_BOW in a Python Counter element. This counter now contains all occuring words together with their frequencies. Its entries can be sorted with the most_common() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove punctuation and numbers\n",
    "\n",
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "print letters_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenization: convert into all lowercase and split into individual words\n",
    "\n",
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()  # Download text data sets, including stop words\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to clean data\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_review = review_to_words( train[\"review\"][0] )\n",
    "print clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print status update every 1000 items that it processes (add this into code above)\n",
    "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "clean_train_reviews = []\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num_reviews )                                                                    \n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print test.shape\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, num_reviews)\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
